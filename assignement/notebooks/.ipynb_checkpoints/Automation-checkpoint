{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6143c9d9-bdc1-43f9-a8f6-fc83cf533081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Koustav Roychowdhury\\AppData\\Local\\Temp\\ipykernel_24212\\2656881898.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df['ch_mergeable'].fillna(2, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "# Load the data\n",
    "change_df = pd.read_csv('../data/t_change.csv')\n",
    "revision_df = pd.read_csv('../data/t_revision.csv')\n",
    "people_df = pd.read_csv('../data/t_people.csv')\n",
    "history_df = pd.read_csv('../data/t_history.csv')\n",
    "file_df = pd.read_csv('../data/t_file.csv')\n",
    "\n",
    "# Convert timestamp columns to datetime for the changes dataset\n",
    "for col in ['ch_createdTime', 'ch_updatedTime']:\n",
    "    if col in change_df.columns:\n",
    "        change_df[col] = pd.to_datetime(change_df[col])\n",
    "\n",
    "# Convert timestamp columns to datetime for the revisions dataset\n",
    "for col in ['rev_createdTime', 'rev_committedTime']:\n",
    "    if col in revision_df.columns:\n",
    "        revision_df[col] = pd.to_datetime(revision_df[col])\n",
    "\n",
    "# Convert timestamp columns to datetime for the history dataset\n",
    "if 'hist_createdTime' in history_df.columns:\n",
    "    history_df['hist_createdTime'] = pd.to_datetime(history_df['hist_createdTime'])\n",
    "\n",
    "# Merging Revision and Comment counts and People\n",
    "# Calculating Counts\n",
    "rev_counts_ab = revision_df.groupby('rev_changeId').size().reset_index(name='revision_count')\n",
    "comment_counts_ab = history_df.groupby('hist_changeId').size().reset_index(name='comment_count')\n",
    "\n",
    "# Merging reviews\n",
    "m1 = pd.merge(change_df, rev_counts_ab, left_on='id', right_on='rev_changeId', how='left')\n",
    "m1['revision_count'] = m1['revision_count'].fillna(0).astype(int)\n",
    "\n",
    "# Merging Comments\n",
    "m2 = pd.merge(m1, comment_counts_ab, left_on='id', right_on='hist_changeId', how='left')\n",
    "\n",
    "# Merging People\n",
    "all_df = pd.merge(m2, people_df, left_on='ch_authorAccountId', right_on='p_accountId', how='left')\n",
    "\n",
    "# Calculating normal metrics\n",
    "# Calculatin time to merge\n",
    "all_df['time_to_merge'] = (all_df['ch_updatedTime'] - all_df['ch_createdTime']).dt.total_seconds() / 3600  # in hours\n",
    "\n",
    "# Forming grouped data\n",
    "normal_metrics_total = all_df.groupby( 'ch_status').agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "normal_metrics_total.insert(loc=0, column='ch_project', value='000-All')\n",
    "normal_metrics_total.insert(loc=2, column='ch_branch', value='000-All')\n",
    "\n",
    "normal_metrics_all = all_df.groupby(['ch_project', 'ch_status']).agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "normal_metrics_all.insert(loc=2, column='ch_branch', value='000-All')\n",
    "\n",
    "normal_metrics_br = all_df.groupby(['ch_project', 'ch_status', 'ch_branch']).agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "normal_metrics = pd.concat([normal_metrics_total, normal_metrics_all, normal_metrics_br], ignore_index=True)\n",
    "\n",
    "\n",
    "# Calculating mergeable DF\n",
    "all_df['ch_mergeable'].fillna(2, inplace=True)\n",
    "\n",
    "mergeable_total = all_df.groupby('ch_status').agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "mergeable_total.insert(loc=0, column='ch_project', value='000-All')\n",
    "mergeable_total.insert(loc=2, column='ch_mergeable', value='000-All')\n",
    "mergeable_total.insert(loc=3, column='ch_branch', value='000-All')\n",
    "\n",
    "\n",
    "mergeable_total_mergeable = all_df.groupby(['ch_status', 'ch_mergeable']).agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "mergeable_total_mergeable.insert(loc=0, column='ch_project', value='000-All')\n",
    "mergeable_total_mergeable.insert(loc=3, column='ch_branch', value='000-All')\n",
    "\n",
    "\n",
    "mergeable_all = all_df.groupby(['ch_project', 'ch_status', 'ch_mergeable']).agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "mergeable_all.insert(loc=3, column='ch_branch', value='000-All')\n",
    "\n",
    "mergeable_br = all_df.groupby(['ch_project', 'ch_status', 'ch_mergeable','ch_branch']).agg(\n",
    "    status_count = ('ch_status', 'count'),\n",
    "    rev_count = ('revision_count', 'sum'),\n",
    "    comment_count = ('comment_count', 'sum'),\n",
    "    avg_time_hrs = ('time_to_merge', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "mergeable_metrics = pd.concat([mergeable_total, mergeable_total_mergeable, mergeable_all, mergeable_br], ignore_index=True)\n",
    "\n",
    "# Calculating Top Authors\n",
    "top_author_total = pd.pivot_table(all_df, \n",
    "                                index='p_name', \n",
    "                                columns='ch_status', \n",
    "                                aggfunc='size', \n",
    "                                fill_value=0).reset_index()\n",
    "top_author_total.insert(loc=0, column='ch_project', value='000-All')\n",
    "top_author_total.insert(loc=2, column='ch_branch', value='000-All')\n",
    "\n",
    "top_author_all = pd.pivot_table(all_df, \n",
    "                                index=['ch_project', 'p_name'], \n",
    "                                columns='ch_status', \n",
    "                                aggfunc='size', \n",
    "                                fill_value=0).reset_index()\n",
    "top_author_all.insert(loc=2, column='ch_branch', value='000-All')\n",
    "\n",
    "top_author_br = pd.pivot_table(all_df, \n",
    "                                index=['ch_project', 'p_name', 'ch_branch'], \n",
    "                                columns='ch_status', \n",
    "                                aggfunc='size', \n",
    "                                fill_value=0).reset_index()\n",
    "\n",
    "top_author = pd.concat([top_author_total, top_author_all, top_author_br], ignore_index=True)\n",
    "\n",
    "# Creating Dimension Tables\n",
    "project_unique = normal_metrics['ch_project'].unique()\n",
    "project_unique = pd.DataFrame(project_unique)\n",
    "branch_unique = normal_metrics.drop_duplicates(subset=['ch_project', 'ch_branch'])\n",
    "branch_unique = pd.DataFrame(branch_unique[['ch_project', 'ch_branch']])\n",
    "\n",
    "\n",
    "# User Table\n",
    "user_df = all_df.groupby('p_name').agg(\n",
    "    # status_count=('ch_status', 'count'),\n",
    "    rev_count=('revision_count', 'sum'),\n",
    "    comment_count=('comment_count', 'sum'),\n",
    "    avg_time_hrs=('time_to_merge', 'mean'),\n",
    "    merged_count=('ch_status', lambda x: (x == 'MERGED').sum()),\n",
    "    abandoned_count=('ch_status', lambda x: (x == 'ABANDONED').sum()),\n",
    "    new_count=('ch_status', lambda x: (x == 'NEW').sum()),\n",
    "    unique_projects=('ch_project', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "user_df['abandonment_rate'] = (user_df['abandoned_count'] / (user_df['merged_count'] + user_df['abandoned_count'] + user_df['new_count']))*100\n",
    "user_df['merge_rate'] = (user_df['merged_count'] / (user_df['merged_count'] + user_df['abandoned_count'] + user_df['new_count']))*100\n",
    "\n",
    "# Project Efficiency\n",
    "# Average time to merge\n",
    "\n",
    "avg_merge_time = pd.pivot_table(\n",
    "    all_df,\n",
    "    index='ch_project',\n",
    "    columns='ch_status',\n",
    "    values='time_to_merge',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Count of ch_status (correct way!)\n",
    "status_counts = pd.pivot_table(\n",
    "    all_df,\n",
    "    index='ch_project',\n",
    "    columns='ch_status',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Merge both DataFrames\n",
    "eff = pd.merge(avg_merge_time, status_counts, on='ch_project', suffixes=('_avg_time', '_count'))\n",
    "\n",
    "\n",
    "eff.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Create a directory if it doesn't exist\n",
    "folder_path = '~/Downloads/assignement/notebooks/Final_Output'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "# Save the DataFrames as CSV files\n",
    "normal_metrics.to_csv(os.path.join(folder_path, 'normal_metrics.csv'), index=False)\n",
    "mergeable_metrics.to_csv(os.path.join(folder_path, 'mergeable_metrics.csv'), index=False)\n",
    "top_author.to_csv(os.path.join(folder_path, 'top_author.csv'), index=False)\n",
    "project_unique.to_csv(os.path.join(folder_path, 'project_unique.csv'), index=False)\n",
    "branch_unique.to_csv(os.path.join(folder_path, 'branch_unique.csv'), index=False)\n",
    "user_df.to_csv(os.path.join(folder_path, 'user_data.csv'), index=False)\n",
    "eff.to_csv(os.path.join(folder_path, 'efficiency_data.csv'), index=False)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "duration = end_time - start_time\n",
    "duration_in_seconds = int(duration.total_seconds())\n",
    "print(duration_in_seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
